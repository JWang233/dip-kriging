{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jzw0n\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of image to upscale. Must be a 256x256 PNG.\n",
    "image_name = \"p_1024.png\"\n",
    "dim = 1024\n",
    "\n",
    "def load_image(filename, dim):\n",
    "    with open(image_name, 'rb') as f:\n",
    "        raw_image = tf.image.decode_png(f.read())\n",
    "\n",
    "    converted = tf.image.convert_image_dtype(\n",
    "        raw_image,\n",
    "        tf.float32,\n",
    "        saturate=True\n",
    "    )\n",
    "    \n",
    "    resized = tf.image.resize_images(\n",
    "        images = converted,\n",
    "        size = [dim, dim]\n",
    "    )\n",
    "\n",
    "    resized.set_shape((dim,dim,3))\n",
    "\n",
    "    blur = gblur(tf.expand_dims(resized, 0))\n",
    "\n",
    "    return blur\n",
    "\n",
    "def save_image(filename, image):\n",
    "    converted_img = tf.image.convert_image_dtype(\n",
    "        image,\n",
    "        tf.uint8,\n",
    "        saturate=True)\n",
    "\n",
    "    encoded_img = tf.image.encode_png(converted_img)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(encoded_img.eval())\n",
    "\n",
    "def down_layer(layer):\n",
    "    layer = tf.contrib.layers.conv2d(\n",
    "        inputs=layer,\n",
    "        num_outputs=128,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        padding='SAME',\n",
    "        activation_fn=None)\n",
    "    \n",
    "    layer = tf.contrib.layers.batch_norm(\n",
    "        inputs=layer,\n",
    "        activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "    layer = tf.contrib.layers.conv2d(\n",
    "        inputs=layer,\n",
    "        num_outputs=128,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding='SAME',\n",
    "        activation_fn=None)\n",
    "    \n",
    "    layer = tf.contrib.layers.batch_norm(\n",
    "        inputs=layer,\n",
    "        activation_fn=tf.nn.leaky_relu)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def up_layer(layer):\n",
    "    layer = tf.contrib.layers.batch_norm(\n",
    "        inputs=layer)\n",
    "    \n",
    "    layer = tf.contrib.layers.conv2d(\n",
    "        inputs=layer,\n",
    "        num_outputs=128,\n",
    "        kernel_size=3,\n",
    "        padding='SAME',\n",
    "        activation_fn=None)\n",
    "    \n",
    "    layer = tf.contrib.layers.batch_norm(\n",
    "        inputs=layer,\n",
    "        activation_fn=tf.nn.leaky_relu\n",
    "    )\n",
    "    \n",
    "    layer = tf.contrib.layers.conv2d(\n",
    "        inputs=layer,\n",
    "        num_outputs=3,\n",
    "        kernel_size=1,\n",
    "        padding='SAME',\n",
    "        activation_fn=None)\n",
    "    \n",
    "    layer = tf.contrib.layers.batch_norm(\n",
    "        inputs=layer,\n",
    "        activation_fn=tf.nn.leaky_relu)\n",
    "    \n",
    "    height, width = layer.get_shape()[1:3]\n",
    "    layer = tf.image.resize_images(\n",
    "        images = layer,\n",
    "        size = [height*2, width*2]\n",
    "    )\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def skip(layer):\n",
    "    conv_out = tf.contrib.layers.conv2d(\n",
    "        inputs=layer,\n",
    "        num_outputs=4,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding='SAME',\n",
    "        normalizer_fn = tf.contrib.layers.batch_norm,\n",
    "        activation_fn=tf.nn.leaky_relu)\n",
    "\n",
    "    return conv_out\n",
    "\n",
    "# Code from https://stackoverflow.com/a/29731818\n",
    "def gkern(kernlen=5, nsig=3):\n",
    "    \"\"\"Returns a 2D Gaussian kernel array.\"\"\"\n",
    "\n",
    "    interval = (2*nsig+1.)/(kernlen)\n",
    "    x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n",
    "    kern1d = np.diff(st.norm.cdf(x))\n",
    "    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n",
    "    kernel = kernel_raw/kernel_raw.sum()\n",
    "    return (tf.convert_to_tensor(kernel, dtype=tf.float32))\n",
    "\n",
    "# Apply the gaussian kernel to each channel to give the image a\n",
    "# gaussian blur.\n",
    "def gblur(layer):\n",
    "    gaus_filter = tf.expand_dims(tf.stack([gkern(),gkern(),gkern()], axis=2), axis=3)\n",
    "    return tf.nn.depthwise_conv2d(layer, gaus_filter, strides=[1,1,1,1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jzw0n\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Shape after downsample: (1, 32, 32, 128)\n",
      "Shape after upsample: (1, 1024, 1024, 3)\n",
      "Output shape: (1, 1024, 1024, 3)\n",
      "WARNING:tensorflow:From C:\\Users\\jzw0n\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# The number of down sampling and up sampling layers.\n",
    "# These should be equal if the ouput and input images\n",
    "# are to be equal.\n",
    "down_layer_count = 5\n",
    "up_layer_count = 5\n",
    "\n",
    "image = load_image(image_name, dim)\n",
    "\n",
    "rand = tf.placeholder(shape=(1,dim,dim,32), dtype=tf.float32)\n",
    "\n",
    "# TODO: test if 32 channels improves performance\n",
    "out = tf.constant(np.random.uniform(0, 0.1, size=(1,dim,dim,32)), dtype=tf.float32) + rand\n",
    "\n",
    "# Connect up all the downsampling layers.\n",
    "skips = []\n",
    "for i in range(down_layer_count):\n",
    "    out = down_layer(out)\n",
    "    # Keep a list of the skip layers, so they can be connected\n",
    "    # to the upsampling layers.\n",
    "    skips.append(skip(out))\n",
    "\n",
    "print(\"Shape after downsample: \" + str(out.get_shape()))\n",
    "\n",
    "# Connect up the upsampling layers, from smallest to largest.\n",
    "skips.reverse()\n",
    "for i in range(up_layer_count):\n",
    "    if i == 0:\n",
    "        # As specified in the paper, the first upsampling layers is connected to\n",
    "        # the last downsampling layer through a skip layer.\n",
    "        out = up_layer(skip(out))\n",
    "    else:\n",
    "        # The output of the rest of the skip layers is concatenated onto\n",
    "        # the input of each upsampling layer.\n",
    "        # Note: It's not clear from the paper if concat is the right operator\n",
    "        # but nothing else makes sense for the shape of the tensors.\n",
    "        out = up_layer(tf.concat([out, skips[i]], axis=3))\n",
    "        \n",
    "print(\"Shape after upsample: \" + str(out.get_shape()))\n",
    "\n",
    "# Restore original image dimensions and channels\n",
    "out = tf.contrib.layers.conv2d(\n",
    "    inputs=out,\n",
    "    num_outputs=3,\n",
    "    kernel_size=1,\n",
    "    stride=1,\n",
    "    padding='SAME',\n",
    "    activation_fn=tf.nn.sigmoid)\n",
    "print(\"Output shape: \" + str(out.get_shape()))\n",
    "\n",
    "E = tf.losses.mean_squared_error(image, gblur(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jzw0n\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(E)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "# sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "save_image(\"output/corrupt.png\", tf.reshape(image, (dim,dim,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.062213805\n",
      "1 0.06306828\n",
      "2 0.05532633\n",
      "3 0.052114535\n",
      "4 0.04965094\n",
      "5 0.04752429\n",
      "6 0.0455149\n",
      "7 0.043586913\n",
      "8 0.041747\n",
      "9 0.039974142\n",
      "10 0.038252525\n",
      "11 0.036609795\n",
      "12 0.035039622\n",
      "13 0.03350573\n",
      "14 0.032038577\n",
      "15 0.030592503\n",
      "16 0.029201003\n",
      "17 0.027843602\n",
      "18 0.026480563\n",
      "19 0.025171846\n",
      "20 0.023877548\n",
      "21 0.022618607\n",
      "22 0.021416314\n",
      "23 0.020248797\n",
      "24 0.019145017\n",
      "25 0.018097866\n",
      "26 0.01709207\n",
      "27 0.01612133\n",
      "28 0.01518657\n",
      "29 0.014298611\n",
      "30 0.013438412\n",
      "31 0.0126008885\n",
      "32 0.0118172765\n",
      "33 0.01105652\n",
      "34 0.0103618605\n",
      "35 0.009684843\n",
      "36 0.009024128\n",
      "37 0.008432859\n",
      "38 0.007853185\n",
      "39 0.007313764\n",
      "40 0.006799038\n",
      "41 0.006312374\n",
      "42 0.0058660824\n",
      "43 0.0054484536\n",
      "44 0.0050557554\n",
      "45 0.004680916\n",
      "46 0.0043452266\n",
      "47 0.004055414\n",
      "48 0.0037935723\n",
      "49 0.0035267973\n",
      "50 0.0032440338\n",
      "51 0.0030310052\n",
      "52 0.0028109003\n",
      "53 0.002616557\n",
      "54 0.0024640542\n",
      "55 0.0022826188\n",
      "56 0.0021780003\n",
      "57 0.0020144666\n",
      "58 0.001924842\n",
      "59 0.0018190831\n",
      "60 0.0017273572\n",
      "61 0.0016631815\n",
      "62 0.0015706797\n",
      "63 0.0015037619\n",
      "64 0.0014478355\n",
      "65 0.0013962669\n",
      "66 0.0013529756\n",
      "67 0.0013060182\n",
      "68 0.0012782012\n",
      "69 0.0012346838\n",
      "70 0.0012087413\n",
      "71 0.0011883507\n",
      "72 0.0011748237\n",
      "73 0.0011420554\n",
      "74 0.0011350008\n",
      "75 0.0011107234\n",
      "76 0.0010877777\n",
      "77 0.0010753851\n",
      "78 0.0010522623\n",
      "79 0.0010381046\n",
      "80 0.0010233424\n",
      "81 0.0010078949\n",
      "82 0.0009905903\n",
      "83 0.0009850574\n",
      "84 0.0009877203\n",
      "85 0.00096436543\n",
      "86 0.0009563086\n",
      "87 0.00093296776\n",
      "88 0.00092175946\n",
      "89 0.00091690273\n",
      "90 0.0009115474\n",
      "91 0.00089157064\n",
      "92 0.00088655227\n",
      "93 0.000865736\n",
      "94 0.00085834955\n",
      "95 0.0008512896\n",
      "96 0.0008417128\n",
      "97 0.00082789914\n",
      "98 0.0008230233\n",
      "99 0.00080417143\n",
      "100 0.00079801696\n",
      "101 0.00079794176\n",
      "102 0.0007863282\n",
      "103 0.0007740616\n",
      "104 0.00076868525\n",
      "105 0.0007560975\n",
      "106 0.00074966275\n",
      "107 0.0007506889\n",
      "108 0.0007385441\n",
      "109 0.00072213676\n",
      "110 0.0007157659\n",
      "111 0.000707535\n",
      "112 0.0007088064\n",
      "113 0.00070235034\n",
      "114 0.00069631106\n",
      "115 0.00068383734\n",
      "116 0.00068804604\n",
      "117 0.0006800951\n",
      "118 0.00067155395\n",
      "119 0.0006687013\n",
      "120 0.00065514183\n",
      "121 0.00065157376\n",
      "122 0.0006413209\n",
      "123 0.00064336037\n",
      "124 0.0006401849\n",
      "125 0.0006284726\n",
      "126 0.00062724686\n",
      "127 0.0006197092\n",
      "128 0.0006180586\n",
      "129 0.00061001774\n",
      "130 0.0006041394\n",
      "131 0.0006020129\n",
      "132 0.00059536315\n",
      "133 0.00059064553\n",
      "134 0.00059046154\n",
      "135 0.0005821321\n",
      "136 0.0005793356\n",
      "137 0.0005737988\n",
      "138 0.00056967646\n",
      "139 0.0005664796\n",
      "140 0.000565775\n",
      "141 0.00056052004\n",
      "142 0.0005575014\n",
      "143 0.0005495912\n",
      "144 0.00055016397\n",
      "145 0.00054737116\n",
      "146 0.0005430532\n",
      "147 0.0005384269\n",
      "148 0.0005395483\n",
      "149 0.0005318024\n",
      "150 0.0005381156\n",
      "151 0.00052862667\n",
      "152 0.00052496087\n",
      "153 0.00052267296\n",
      "154 0.0005120626\n",
      "155 0.0005114157\n",
      "156 0.00050746225\n",
      "157 0.00050874933\n",
      "158 0.0005040039\n",
      "159 0.00050537003\n",
      "160 0.0005008139\n",
      "161 0.000498259\n",
      "162 0.0004974478\n",
      "163 0.00049222284\n",
      "164 0.0004873109\n",
      "165 0.00048625472\n",
      "166 0.0004838576\n",
      "167 0.0004820254\n",
      "168 0.0004763502\n",
      "169 0.00047874497\n",
      "170 0.00047667694\n",
      "171 0.0004752834\n",
      "172 0.00047267156\n",
      "173 0.00047032294\n",
      "174 0.0004660984\n",
      "175 0.00046190992\n",
      "176 0.000462219\n",
      "177 0.00045973025\n",
      "178 0.00045990155\n",
      "179 0.00045173426\n",
      "180 0.00045071045\n",
      "181 0.0004598157\n",
      "182 0.00046625608\n",
      "183 0.00046725597\n",
      "184 0.00044847338\n",
      "185 0.000442045\n",
      "186 0.0004549988\n",
      "187 0.00045287295\n",
      "188 0.00043736308\n",
      "189 0.000434057\n",
      "190 0.00044405946\n",
      "191 0.00043451125\n",
      "192 0.00042902472\n",
      "193 0.00042916392\n",
      "194 0.00042978223\n",
      "195 0.00042276122\n",
      "196 0.00042854509\n",
      "197 0.00042412477\n",
      "198 0.0004159999\n",
      "199 0.00041929027\n",
      "200 0.00041898506\n",
      "201 0.0004053585\n",
      "202 0.00041413415\n",
      "203 0.00041284345\n",
      "204 0.00040727548\n",
      "205 0.00040815034\n",
      "206 0.00041171815\n",
      "207 0.0004051253\n",
      "208 0.00039873086\n",
      "209 0.00040045826\n",
      "210 0.00039465772\n",
      "211 0.00039202752\n",
      "212 0.00039149492\n",
      "213 0.0003886386\n",
      "214 0.00038828465\n",
      "215 0.00038936967\n",
      "216 0.0003869135\n",
      "217 0.00038244188\n",
      "218 0.00038531885\n",
      "219 0.0003820903\n",
      "220 0.0003794098\n",
      "221 0.0003740026\n",
      "222 0.00038100683\n",
      "223 0.00038052074\n",
      "224 0.00037615237\n",
      "225 0.00037202335\n",
      "226 0.0003696528\n",
      "227 0.00037930525\n",
      "228 0.00036979027\n",
      "229 0.00036309622\n",
      "230 0.0003685799\n",
      "231 0.00038404853\n",
      "232 0.00037433105\n",
      "233 0.00036140173\n",
      "234 0.00036069736\n",
      "235 0.00036166565\n",
      "236 0.00035669247\n",
      "237 0.00035098425\n",
      "238 0.00035353846\n",
      "239 0.00035791795\n",
      "240 0.00035315775\n",
      "241 0.0003478547\n",
      "242 0.0003480414\n",
      "243 0.0003481464\n",
      "244 0.00034802846\n",
      "245 0.00034449503\n",
      "246 0.0003401864\n",
      "247 0.00034844584\n",
      "248 0.0003436232\n",
      "249 0.0003402361\n",
      "250 0.0003344446\n",
      "251 0.00033483354\n",
      "252 0.0003377313\n",
      "253 0.000339184\n",
      "254 0.00033480243\n",
      "255 0.00033151542\n",
      "256 0.00033438695\n",
      "257 0.000330755\n",
      "258 0.0003343655\n",
      "259 0.00032777537\n",
      "260 0.00032843425\n",
      "261 0.00032605056\n",
      "262 0.00032578537\n",
      "263 0.00032473527\n",
      "264 0.0003219644\n",
      "265 0.00032201433\n",
      "266 0.00032318532\n",
      "267 0.00032034703\n",
      "268 0.0003250429\n",
      "269 0.00032608738\n",
      "270 0.00033405994\n",
      "271 0.00035189456\n",
      "272 0.00036531084\n",
      "273 0.0003505298\n",
      "274 0.00031901474\n",
      "275 0.0003219823\n",
      "276 0.00034420114\n",
      "277 0.00032758972\n",
      "278 0.0003104881\n",
      "279 0.00032947434\n",
      "280 0.00031909443\n",
      "281 0.00030863288\n",
      "282 0.00031825973\n",
      "283 0.0003117479\n",
      "284 0.00030418584\n",
      "285 0.00031237994\n",
      "286 0.00030912485\n",
      "287 0.00030162238\n",
      "288 0.00030686538\n",
      "289 0.00030518588\n",
      "290 0.00029860265\n",
      "291 0.00030127578\n",
      "292 0.000300243\n",
      "293 0.00029609658\n",
      "294 0.00029913065\n",
      "295 0.00029853007\n",
      "296 0.00029494116\n",
      "297 0.000295742\n",
      "298 0.0002967674\n",
      "299 0.0002891094\n",
      "300 0.00029290325\n",
      "301 0.00029127297\n",
      "302 0.0002891193\n",
      "303 0.00028755088\n",
      "304 0.0002892936\n",
      "305 0.00028952133\n",
      "306 0.0002878138\n",
      "307 0.0002830988\n",
      "308 0.0002819874\n",
      "309 0.00028516672\n",
      "310 0.00028480907\n",
      "311 0.00028376057\n",
      "312 0.00028498992\n",
      "313 0.00027925102\n",
      "314 0.0002793551\n",
      "315 0.00028004765\n",
      "316 0.00027732193\n",
      "317 0.00027891647\n",
      "318 0.00027954546\n",
      "319 0.00027725386\n",
      "320 0.00027908484\n",
      "321 0.0002798414\n",
      "322 0.00027649806\n",
      "323 0.0002770933\n",
      "324 0.00027368314\n",
      "325 0.0002717897\n",
      "326 0.00027188353\n",
      "327 0.00027019955\n",
      "328 0.0002672611\n",
      "329 0.00026897353\n",
      "330 0.000268711\n",
      "331 0.00026751676\n",
      "332 0.00026811837\n",
      "333 0.00026786697\n",
      "334 0.0002690441\n",
      "335 0.0002639733\n",
      "336 0.00026529116\n",
      "337 0.00026591338\n",
      "338 0.00026950383\n",
      "339 0.00027887747\n",
      "340 0.00030222491\n",
      "341 0.0003371101\n",
      "342 0.0003310251\n",
      "343 0.0002816556\n",
      "344 0.00027205647\n",
      "345 0.00030093317\n",
      "346 0.00027767534\n",
      "347 0.00026884043\n",
      "348 0.00028961882\n",
      "349 0.00026760116\n",
      "350 0.0002720127\n",
      "351 0.0002786255\n",
      "352 0.00026312438\n",
      "353 0.0002721348\n",
      "354 0.00026681536\n",
      "355 0.0002621369\n",
      "356 0.00026927065\n",
      "357 0.00025986606\n",
      "358 0.00026049255\n",
      "359 0.0002599495\n",
      "360 0.00025584138\n",
      "361 0.00025724355\n",
      "362 0.0002561988\n",
      "363 0.00025423776\n",
      "364 0.0002540151\n",
      "365 0.00025327303\n",
      "366 0.0002539998\n",
      "367 0.00025543073\n",
      "368 0.00025268595\n",
      "369 0.00024857177\n",
      "370 0.0002552657\n",
      "371 0.00024962114\n",
      "372 0.0002486892\n",
      "373 0.00024903732\n",
      "374 0.0002439058\n",
      "375 0.00024902934\n",
      "376 0.0002469709\n",
      "377 0.0002439581\n",
      "378 0.00024738236\n",
      "379 0.0002459597\n",
      "380 0.0002444012\n",
      "381 0.00024312337\n",
      "382 0.00024241302\n",
      "383 0.00024148793\n",
      "384 0.00024228061\n",
      "385 0.00023985865\n",
      "386 0.00024208358\n",
      "387 0.00024111227\n",
      "388 0.0002417668\n",
      "389 0.00024076749\n",
      "390 0.00023920058\n",
      "391 0.00023782787\n",
      "392 0.00023884507\n",
      "393 0.0002387934\n",
      "394 0.00023486368\n",
      "395 0.00023723661\n",
      "396 0.00023696381\n",
      "397 0.00023901078\n",
      "398 0.0002367872\n",
      "399 0.00023351963\n",
      "400 0.0002346713\n",
      "401 0.00023334926\n",
      "402 0.0002329783\n",
      "403 0.00023303118\n",
      "404 0.00023226325\n",
      "405 0.00023337017\n",
      "406 0.0002347661\n",
      "407 0.00023599126\n",
      "408 0.00023864402\n",
      "409 0.00023698069\n",
      "410 0.00023285752\n",
      "411 0.00022993109\n",
      "412 0.00022951596\n",
      "413 0.00022874588\n",
      "414 0.00023239267\n",
      "415 0.00023214438\n",
      "416 0.00023008278\n",
      "417 0.00022804934\n",
      "418 0.00022689173\n",
      "419 0.00022636059\n",
      "420 0.00022663303\n",
      "421 0.00022913296\n",
      "422 0.00023158855\n",
      "423 0.00022805195\n",
      "424 0.00022419558\n",
      "425 0.00022233435\n",
      "426 0.00022515804\n",
      "427 0.00022530404\n",
      "428 0.0002256524\n",
      "429 0.00022713479\n",
      "430 0.00022665293\n",
      "431 0.00022223585\n",
      "432 0.00022042364\n",
      "433 0.00022212876\n",
      "434 0.00021871385\n",
      "435 0.00022101373\n",
      "436 0.0002215347\n",
      "437 0.00022015373\n",
      "438 0.00021983297\n",
      "439 0.00021966825\n",
      "440 0.0002170921\n",
      "441 0.00021800237\n",
      "442 0.00021764945\n",
      "443 0.00021995157\n",
      "444 0.00021986804\n",
      "445 0.00022045466\n",
      "446 0.00022054948\n",
      "447 0.00022359338\n",
      "448 0.00022351481\n",
      "449 0.00022769743\n",
      "450 0.00022908811\n",
      "451 0.00022910198\n",
      "452 0.00022451031\n",
      "453 0.00021889144\n",
      "454 0.00021483196\n",
      "455 0.00021488265\n",
      "456 0.0002161744\n",
      "457 0.00022204618\n",
      "458 0.00021963724\n",
      "459 0.00021578865\n",
      "460 0.00021093285\n",
      "461 0.0002094489\n",
      "462 0.00021345394\n",
      "463 0.00021312521\n",
      "464 0.00021666421\n",
      "465 0.00021524553\n",
      "466 0.000209941\n",
      "467 0.00020730162\n",
      "468 0.00021118716\n",
      "469 0.00021011439\n",
      "470 0.00021033913\n",
      "471 0.00021363418\n",
      "472 0.00021152616\n",
      "473 0.00020865271\n",
      "474 0.00020604063\n",
      "475 0.00020925369\n",
      "476 0.00021151343\n",
      "477 0.00020695227\n",
      "478 0.00020663002\n",
      "479 0.00020771961\n",
      "480 0.00020474348\n",
      "481 0.00020396832\n",
      "482 0.00020396533\n",
      "483 0.00020297345\n",
      "484 0.00020524791\n",
      "485 0.00020562603\n",
      "486 0.00020437408\n",
      "487 0.00020521601\n",
      "488 0.00020255643\n",
      "489 0.00020052832\n",
      "490 0.00020296592\n",
      "491 0.00020179914\n",
      "492 0.00020099319\n",
      "493 0.00020255236\n",
      "494 0.00020131528\n",
      "495 0.00020015774\n",
      "496 0.00020209061\n",
      "497 0.00020098132\n",
      "498 0.00019834796\n",
      "499 0.00019943471\n",
      "500 0.00019975739\n",
      "501 0.00019929303\n",
      "502 0.00019775068\n",
      "503 0.00019747321\n",
      "504 0.00020017923\n",
      "505 0.00020612415\n",
      "506 0.00022061235\n",
      "507 0.00024403517\n",
      "508 0.00029149\n",
      "509 0.00029947225\n",
      "510 0.00025711255\n",
      "511 0.00020385733\n",
      "512 0.00025354166\n",
      "513 0.0002505861\n",
      "514 0.00020352128\n",
      "515 0.00025444405\n",
      "516 0.00022264663\n",
      "517 0.0002132147\n",
      "518 0.00023736857\n",
      "519 0.00020022418\n",
      "520 0.00022640638\n",
      "521 0.0002020662\n",
      "522 0.00021535657\n",
      "523 0.00021015455\n",
      "524 0.0002036678\n",
      "525 0.00021062478\n",
      "526 0.0001984327\n",
      "527 0.0002098899\n",
      "528 0.00019798195\n",
      "529 0.00020719721\n",
      "530 0.0002002667\n",
      "531 0.00020069268\n",
      "532 0.00020072528\n",
      "533 0.00019826984\n",
      "534 0.0001993185\n",
      "535 0.00019342169\n",
      "536 0.00019556932\n",
      "537 0.0001933161\n",
      "538 0.00019458611\n",
      "539 0.00019373563\n",
      "540 0.00019497432\n",
      "541 0.00019258993\n",
      "542 0.0001913564\n",
      "543 0.00019085307\n",
      "544 0.000190998\n",
      "545 0.00019109924\n",
      "546 0.0001880779\n",
      "547 0.00019005242\n",
      "548 0.00018753113\n",
      "549 0.00018913853\n",
      "550 0.00018958107\n",
      "551 0.0001867956\n",
      "552 0.00018895952\n",
      "553 0.0001881537\n",
      "554 0.0001872903\n",
      "555 0.00018851687\n",
      "556 0.00018683949\n",
      "557 0.00018856818\n",
      "558 0.00018535892\n",
      "559 0.00018357579\n",
      "560 0.0001856557\n",
      "561 0.00018562615\n",
      "562 0.00018357507\n",
      "563 0.00018442835\n",
      "564 0.00018446716\n",
      "565 0.00018290784\n",
      "566 0.00018235542\n",
      "567 0.00018331275\n",
      "568 0.00018355432\n",
      "569 0.0001809392\n",
      "570 0.00018090826\n",
      "571 0.00018152977\n",
      "572 0.00018133047\n",
      "573 0.00018045354\n",
      "574 0.00017946427\n",
      "575 0.00018041489\n",
      "576 0.00017899815\n",
      "577 0.0001795547\n",
      "578 0.0001784602\n",
      "579 0.00017869247\n",
      "580 0.00017661888\n",
      "581 0.00017630857\n",
      "582 0.00017557915\n",
      "583 0.00017716835\n",
      "584 0.000177099\n",
      "585 0.00017591512\n",
      "586 0.00017695893\n",
      "587 0.00017705288\n",
      "588 0.0001760929\n",
      "589 0.00017801092\n",
      "590 0.00017558098\n",
      "591 0.00017523218\n",
      "592 0.00017544568\n",
      "593 0.00017710471\n",
      "594 0.0001763659\n",
      "595 0.00017552033\n",
      "596 0.00017513473\n",
      "597 0.00017679913\n",
      "598 0.00017629676\n",
      "599 0.00017417247\n",
      "600 0.00017626554\n",
      "601 0.00017672719\n",
      "602 0.00017485373\n",
      "603 0.00017506706\n",
      "604 0.00017670805\n",
      "605 0.00017565448\n",
      "606 0.00017276425\n",
      "607 0.00017296842\n",
      "608 0.0001729366\n",
      "609 0.0001713372\n",
      "610 0.00017223087\n",
      "611 0.00017224898\n",
      "612 0.00017218216\n",
      "613 0.00017033426\n",
      "614 0.00017243535\n",
      "615 0.00017168977\n",
      "616 0.00016968866\n",
      "617 0.00016929157\n",
      "618 0.00016969266\n",
      "619 0.00017056573\n"
     ]
    }
   ],
   "source": [
    "for i in range(4001):\n",
    "    # generate noise to learn the image \n",
    "    new_rand = np.random.uniform(0, 1.0/30.0, size=(1,dim,dim,32))\n",
    "    _, lossval = sess.run(\n",
    "        [train_op, E],\n",
    "        feed_dict = {rand: new_rand}\n",
    "    )\n",
    "    if i % 100 == 0:\n",
    "        image_out = sess.run(out, feed_dict={rand: new_rand}).reshape(dim,dim,3)\n",
    "        save_image(\"output/%d_%s\" % (i, image_name), image_out)\n",
    "    # tmp_E = tf.losses.mean_squared_error(image, gblur(image_out))\n",
    "    print(i, lossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
